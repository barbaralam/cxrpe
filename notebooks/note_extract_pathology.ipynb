{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f73fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os; \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['NCCL_DEBUG'] = 'TRACE'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "#; os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "\n",
    "# from inference import create_prompt_with_llama2_chat_format\n",
    "from extract_pathology import vllm_generate, metrics_xr_pathologies_iou, map_to_canonical_names, get_pathology_confidence\n",
    "\n",
    "print(torch.cuda.is_available(), torch.cuda.device_count())\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_pathology import get_argparse\n",
    "from rosemary import jpt_parse_args\n",
    "\n",
    "\n",
    "quantization = None\n",
    "gpu_memory_utilization = .9\n",
    "model_name_or_path = 'results/baselines/unsloth/llama-3-8b'; model_name = 'llama-3-8b'; max_model_len = 8192; use_chat_template = False\n",
    "model_name_or_path = 'results/baselines/google/gemma-2b'; model_name = 'gemma-2b'; max_model_len = 8192; use_chat_template = False; gpu_memory_utilization = .7\n",
    "\n",
    "icl_example_file = 'prompts/classify_pe/examples_v1.json'\n",
    "test_label_file = 'prompts/classify_pe/test_set.json'\n",
    "prompt_template = 'prompts/classify_pe/prompt_icl_simple_instruct.j2'\n",
    "output_dir = os.path.join('results/classify_pe_from_ct_report/', f'{model_name}')\n",
    "\n",
    "\n",
    "cmd = f\"\"\"\n",
    "--test_label_file {test_label_file} \\\n",
    "--icl_example_file {icl_example_file} \\\n",
    "--prompt_template {prompt_template} \\\n",
    "--model_name_or_path {model_name_or_path} \\\n",
    "--max_model_len {max_model_len} \\\n",
    "{'--quantization ' + quantization if quantization else ''} \\\n",
    "--gpu_memory_utilization {gpu_memory_utilization} \\\n",
    "{'--use_chat_template' if use_chat_template else ''} \\\n",
    "--max_tokens 256 \\\n",
    "--torch_dtype float16 \\\n",
    "--output_dir {output_dir}\n",
    "\"\"\"\n",
    "\n",
    "print(cmd)\n",
    "\n",
    "parser = get_argparse()\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = vllm.LLM(\n",
    "    model=args.model_name_or_path,\n",
    "    tokenizer=args.model_name_or_path,\n",
    "    tokenizer_mode=\"auto\",\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    dtype=getattr(torch, args.torch_dtype) if args.torch_dtype else 'auto',\n",
    "    max_model_len=args.max_model_len,\n",
    "    enable_prefix_caching=True,\n",
    "    quantization=args.quantization,\n",
    "    gpu_memory_utilization=args.gpu_memory_utilization,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = model.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58519448",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.icl_example_file, 'r') as f:\n",
    "    examples = json.load(f)\n",
    "    \n",
    "print([x['label'] for x in examples])\n",
    "examples = examples[2:]\n",
    "print([x['label'] for x in examples])\n",
    "    \n",
    "random.seed(0)\n",
    "random.shuffle(examples)\n",
    "print(f\"#In-context examples: {len(examples)}\")\n",
    "\n",
    "with open(args.prompt_template, 'r') as f:\n",
    "    prompt_template = Template(f.read())\n",
    "prompt_prefix = prompt_template.render(examples=examples)\n",
    "print(f\"Prompt prefix (inclouding icl examples) #Tokens: {len(tokenizer(prompt_prefix)['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.test_label_file, 'r') as f:\n",
    "    data_true = json.load(f)\n",
    "    \n",
    "print(f\"Test set size: {len(data_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321905e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for example in data_true:\n",
    "    prompt = prompt_template.render(examples=examples + [{'report': example['report']}])\n",
    "    prompts.append(prompt)\n",
    "    \n",
    "\n",
    "if args.use_chat_template:\n",
    "    prompts = [{'role': 'user', 'content': x} for x in prompts]\n",
    "    if 'llama-2' in args.model_name_or_path.lower():\n",
    "        prompts = [create_prompt_with_llama2_chat_format(x, tokenizer) for x in prompts]\n",
    "    else:\n",
    "        prompts = [tokenizer.apply_chat_template(x, tokenize=False, add_generation_prompt=True) for x in prompts]\n",
    "    \n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc265d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = vllm.SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=args.max_tokens,\n",
    "    stop=[\"#####\"],\n",
    ")\n",
    "start = time.time()\n",
    "# prefix cached after first batch is processed, so need to call generate once to calculate the prefix and cache it\n",
    "outputs = vllm_generate(prompts[:1], model, sampling_params)\n",
    "outputs = vllm_generate(prompts[:len(prompts)], model, sampling_params)\n",
    "elapsed = time.time()-start\n",
    "\n",
    "print(f'model.generate() elapsed: {elapsed:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "output_reference = True\n",
    "\n",
    "for i, example in enumerate(data_true):\n",
    "    acc = example['accession_number']\n",
    "\n",
    "    output = outputs[i]\n",
    "    try:\n",
    "        # more robust parsing, e.g., ignore rows that have not finished generating\n",
    "        output_eval = [eval(x.strip()) for x in output.split('- ') if x!='' and x.count('\"')%2==0]\n",
    "        output_eval = [x for x in output_eval if len(x)==(3 if output_reference else 2)]\n",
    "        if any([len(x)!=(3 if output_reference else 2) for x in output_eval]):\n",
    "            raise\n",
    "        if output_reference:\n",
    "            output_formatted = [{'reference': x[0], 'pathology': x[1], 'confidence': x[2]} for x in output_eval]\n",
    "        else:\n",
    "            output_formatted = [{'pathology': x[0], 'confidence': x[1]} for x in output_eval]\n",
    "    except:\n",
    "        print(f'==== output cannot be evaluated/formatted properly [{i}] '+acc+'\\n')\n",
    "        print(output)\n",
    "        output_eval = output\n",
    "        output_formatted = []\n",
    "\n",
    "    data.append({\n",
    "        'accession_number': acc,\n",
    "        'report': example['report'],\n",
    "        'output': output,\n",
    "        'output_formatted': output_formatted,\n",
    "    })\n",
    "    \n",
    "print(f\"#Examples that cannot be parsed from model generation: {sum(x['output_formatted']==[] for x in data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0674c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cxrpe] *",
   "language": "python",
   "name": "conda-env-cxrpe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
